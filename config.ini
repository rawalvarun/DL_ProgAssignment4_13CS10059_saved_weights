[model_initialization_hyperparameters]
batch_size = 20

num_steps = 35 
# number of unrolled time steps

hidden_size = 450 
# number of blocks in an LSTM cell

vocab_size = 10000

max_grad_norm = 5 
# maximum gradient for clipping

init_scale = 0.05 
# scale between -0.1 and 0.1 for all random initialization


num_layers = 2 
# number of LSTM layers


[model_training_hyperparameters]

keep_prob = 0.5 
# dropout probability

learning_rate = 1.0
lr_decay = 0.8

lr_decay_epoch_offset = 6 
# don't decay until after the Nth epoch

[general_interface_options]

enable_training = 0
# SET to 1 if want to continue training from last checkpoint || to 0 if just testing is required

verbose_mode = 0
# SET TO either 1 for printing the details (to enable verbose mode) while running the script || 0 to disable it
